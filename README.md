# Persistent_Memory

## Introduction

It is widely acknowledged that application performance and security are undeniable part of high-performance systems in dataenters. Hence, there are numerous number of contributions in various layers of high-performance systems that some of them are in infancy phase and rest of them are commercialized. In addition, distributed high-performance systems such as database systems or key-value stores which are widely used in production datacenters need to be improved and enhanced in various aspects. For example, there should be quite huge, byte-addressable, persistence and low cost memory systems in high-performance datacenters for applications like distributed databases, and also there should be a secure, reliable and trusted mechanism to completely isolate applications' code and data from each other in order to prevent the occurrence of any kinds of attacks. In recent research, Intel proposed two products to address abovementioned issues: Intel Optane DC Persistent Memory Module(DCPMM) which brings this vision closer to reality, and Intel Software Guard Extension (SGX). Intel Optane Persistent Memory (PMEM) promises byte-addressability, persistence, high capacity (128/256/512GB per DIMM), lower latency in read/write, lower cost than DRAMs and high performance, all on memory bus\cite{dash}\cite{snia}\cite{empirical}. Along with that, Intel SGX enables user-level code to create private memory regions called enclaves, whose code and data are protected by the CPU from any software and hardware attacks outside the enclaves. In fact, SGX provides a practical solution to the problem of secure computation on untrusted platforms such as multi tenant datacenters, public clouds, etc\cite{occlum}.

Furthermore, with the emergence of Remote Persistent Memory (RPM), the developers are persuaded to implemented high-performance distributed systems on RPM with specific remote memory access called Remote Direct Memory Access (RDMA). RDMA-based distributed systems uses specific kinds of NICs called RNIC which eliminates TCP/IP protocol stack for network connectivity. Therefore, it is not possible to implement high-performance distributed systems over commodity hardware. The problem gets even more complex when so called systems need strong security mechanisms to have confidentiality, integrity and freshness in untrusted environment. Some contributions such as \cite{eRPC} and \cite{dspm} proposed a mechanism for distributed systems to access resources remotely with latency and performance better or comparable with specialized hardware. Yet, there are several work in these mechanisms need to be added such as security to be more trustable in production environments. As a result, there is a fundamental need to provide an infrastructure for developers to implement a secure and distributed high performance systems over commodity hardwares inside datacenters. 



---
It is widely acknowledged that network latency has not been improved drastically compared to other computer performance metrics. It has also been a heated debates among researchers and developers that it should be possible to achieve end-to-end remote procedure call (RPC) latencies of 5-10$\mu$s in large datacenters using commodity hardware and software \cite{lowlatency}. Therefore, we can see different contributions to improve latency. Some contributions have tried to manipulate current running protocols such as \cite{dctcp}, while some other have tried to propose totally different hardware infrastructure to provide low latency feature such as[RDMA], and some of them have tried to optimize the way applications perform such as \cite{eRPC}. Besides, latency can be treated as a networked oriented or system oriented parameter. It means that the latency parameter can be improved either by improving network protocols itself, or by improving commodity hardware or software systems running on the traditional network. Therefore, we have been witnessing a tremendous achievements in both sides, specifically in memory systems.
Recent developments on byte-addressable Persistent Memories(PMEMs) allows application developers to combine storage and memory into a single layer to achieve native or near native performance\cite{snia}. With PMEMs, servers can equip terabytes of memory that survive power outages, and all of this persistent capacity can be managed with low latency. In other word, PMEMs promise byte-addressability, persistence, high capacity (128/256/512GB per DIMM), lower cost than DRAMs and high performance, all on memory bus\cite{dash}. The technology allows applications to access persistent data using load/store instructions, avoiding the need for a block-based interface utilized by traditional storage systems. The recent release of Intel Optane DC Persistent Memory Module(DCPMM) bring this vision closer to reality. If we see datacenter systems through the memory perspective, memory can be categorized as Local Memories and Remote Memories. Local memories refer to memories which reside near in CPU and are directly connected to it via memory bus. Remote memories, on the other hand, refer to memories reside in remote hosts and can be achieved over the network using protocols and mechanisms such as message passing, RDMA and RPC. Therefore, accessing data remotely over remote persistent memory is called Remote Persistent Memory Access(RPMA)\cite{snia}. There are several works and contribution towards implementing distributed persistent memory such as \cite{dspm}. However, it is still unclear that how to best utilized them in a distributed environment with traditional communication infrastructure(TCP/IP) and also strong security and protection. It is also worth mentioning that, different kinds of approaches towards building a secure system have been presented ranging from application to hardware layers. One of the promising technologies in providing security is Intel Software Guard Extension (SGX). SGX extension provides hardware-based isolation and memory encryption to provide more code protection for developers. It aims also to provide confidentiality and integrity guarantee to security-sensitive data and computation where all privileged softwares such as kernel and hypervisors are potentially malicious\cite{scone}\cite{speicher}\cite{enclavedb}.

In this contribution, the ultimate goal is to build a secure remote persistent memory abstraction,
which can be used to build different set of high-performance and secure applications such as KV stores/databases accessible over the traditional TCP/IP network inside datacenters.

### RDMA
 NVMMs appear to mesh perfectly with another popular technology, remote direct memory access (RDMA). RDMA gives a client direct access to memory on a remote machine and mediates this access through a memory region abstraction that handles the necessary translations and permissions. NVMM and RDMA seem eminently compatible: by combining them, we should be able to build network-attached, byte-addressable, persistent storage. Unfortunately, however, the systems were not designed to work together. An NVMM- aware file system manages persistent memory as files, whereas RDMA uses a different abstraction memory regions to organize remotely accessible memory. As a result, in practice, building RDMA-accessible NVMMs requires expensive translation layers resulting from this duplication of effort that spans permissions, naming, and address translation. At first glance, by combining NVMM and RDMA, we could unify storage, memory and network to provide large, stable, byte-addressable network-attached memory. Unfortunately, the existing systems used to manage these technologies are simultaneously overlapping and incompatible.  RDMA merges local and remote memory. RDMA allows a client to directly access memory on a remote server. Once the remote server decides to allow incoming access, it registers a portion of its address space as an RDMA memory region and sends the client a key to access it. Using the key, the client can enlist the server’s RDMA network interface (RNIC) to directly read and write to the server’s memory, bypassing the CPU. RDMA is popular as it offloads most of the networking stack onto hardware and provides close-to-hardware abstractions, exhibiting much better latency compared to TCP/IP protocol.

RPMA – Remote Persistent Memory Access

Latency is a key consideration in choosing a connectivity method for memory or storage. Latency refers to the time it takes to complete an access such as a read, write, load or store. There are two very important latency thresholds that change how applications see storage or memory represented by the background color bands in this figure. These thresholds are used by system designers when implementing access to stored data, to determine whether the access is to be synchronous, polled or asynchronous. In today’s large non-uniform memory access (NUMA) systems, latencies of up to 200 nS are generally considered to be acceptable. NUMA systems must be very responsive because CPU instruction processing on a core or thread is suspended during the memory access. Latencies of more than 200 nS in a memory system quickly pile up, resulting in wasted CPU time. On the other hand, when an application does IO for a storage access that is expected to take more than 2-3 uS, it will usually choose to block a thread or process. The CPU will execute a context switch and make progress on another thread or process until it is notified that the access is complete. For latencies between 200 nS and 2 uS it may be preferable for the CPU to poll for IO completion as this consumes one thread or core but does not slow down the rest of the CPU.

The concept of disaggregated memory is used to illustrate cases where memory that is not contained within a server is still accessed at memory speed. Disaggregated memory still looks like memory to CPU’s. It operates at memory speed in cache line size units and it may be subject to distance limitations to insure sufficiently low latency. Disaggregated memory is made scalable through the use of optical networks such as those based on silicon photonics to increase the distance of memory speed interfaces.
Networked memory is accessed through a high speed network rather than directly through a memory interface. Memory access is achieved over the network using protocols such as message passing and RDMA. Networked persistent memory is not cache coherent with the CPU. Unlike local or disaggregated persistent memory where all of the NVDIMMs can be part of a single system image, the NVDIMMs on a remote node are not part of a single system image.

![picture](data/model.png)

The Optane DIMM is the first scalable, commercially avail- able NVDIMM. Compared to existing storage devices (in- cluding the Optane SSDs) that connect to an external interface such as PCIe, the Optane DIMM has lower latency, higher read bandwidth, and presents a memory address-based inter- face instead of a block-based NVMe interface. Compared to DRAM, it has higher density and persistence. At its debut, the Optane DIMM is available in three different capacities: 128, 256, and 512 GB.
