# Persistent_Memory

## Introduction

Application performance and security are undeniable parts of high-performance systems in datacenters. Hence, there are numerous number of studies in various layers of these systems that some of them are in infancy phase \textcolor{red}{\cite{}} and rest of them are commercialized \textcolor{red}{\cite{}}. In addition, distributed high-performance systems such as database systems or key-value stores which are widely used in production datacenters, need to be improved in various aspects \textcolor{red}{\cite{}}. For example, there should be quite huge, byte-addressable, persistence and low cost memory systems in high-performance datacenters for applications like distributed databases. Also, there should be a secure, reliable and trusted mechanism to completely isolate applications' code and data from each other to prevent the occurrence possible attacks. According to Recent researches in \cite{intelsgx,empirical}, Intel proposed two products to address above mentioned issues: Intel Optane Persistent Memory (PMEM) and Intel Software Guard Extension (SGX). Intel Optane Persistent Memory (PMEM) promises byte-addressability, persistence, high capacity (128/256/512GB per DIMM), lower latency in read/write, lower cost than DRAMs and high performance, all on memory bus \cite{enclavedb, occlum}. Intel SGX enables user-level code to create private memory regions called enclaves, which code and data are protected by the CPU from any software and hardware attacks outside the enclaves. In fact, SGX provides a practical solution to the problem of secure computation on untrusted platforms such as multi tenant datacenters and public clouds \cite{occlum}.

Furthermore, with the emergence of Remote Persistent Memory (RPM), the developers have been studied to implement high-performance distributed systems on RPM with specific remote memory access, called Remote Direct Memory Access (RDMA). RDMA-based distributed systems use specific kinds of NICs, called RNIC which eliminates TCP/IP protocol stack for network connectivity. Therefore, it is not possible to implement high-performance distributed systems over commodity hardware. The problem also gets even more complex when these systems need strong security mechanisms to have confidentiality, integrity and freshness in untrusted environments. Some researches such as \cite{eRPC} and \cite{dspm} proposed mechanisms for distributed systems to access resources remotely with latency comparable to specialized hardware like RDMA. Yet, there are several works need to be added in these mechanisms such as security to prepare the whole system operable in un-trustable environments. As a result, there is a fundamental need to provide an infrastructure for developers to implement a secure and distributed high performance system over commodity hardware inside datacenters. So, the ultimate goal in proposed research is to design and build COSMOS, a Se\underline{c}ure Rem\underline{o}te Per\underline{s}istent Me\underline{mo}ry Ab\underline{s}traction, which can be used to build different set of high-performance and secure applications such as KV stores/databases accessible over the traditional TCP/IP network inside datacenters. To this end, the entire system is divided into three parts: Persistent Memory, Networking infrastructure which leverages the concept and implementation of Extended Remote Procedure Call (eRPC) and security which is provided by Intel SGX.

## Background

In this section, each part of COSMOS which is mentioned in the previous section, will be elaborated in greater detail.

### Persistent Memory

Persistent Memory or PMEM is a type of Non-Volatile memory (NVMem) which is directly connected to memory bus rather than PCI-e. PMEMs provide byte-addressability, persistence, and latency which is within an order of magnitude of DRAM\cite{dspm, empirical,snia}.As shown in Figure\ref{figure:hierarchy}, PMEMs provide a new entry in the memory-storage hierarchy which fills the perfromance/capcacity gap\cite{pmem.io}.

![picture](data/memorypyramid.png)

With PMEMs, applications have a new tier available for data placement. In addition to the memory and storage tiers, the persistent memory tier offers greater capacity than DRAM and significantly faster performance than storage. Applications can access persistent memory like they do with traditional memory, eliminating the need to page blocks of data back and forth between memory and storage.
PMEMs are built to work in two modes: Memory Mode where it acts exactly like DRAMs, and App Direct Mode where it plays the role of persistent memory. In App Direct Mode, software has a byte-addressable way to talk to the persistent memory capacity. In-memory databases restart time can also be significantly reduced because applications no longer have to reload data from storage to main memory, as they can access the persistent memory address space by using load/store accesses.

With the emergence of Intel's Optane Dual Inline Memory Module (DIMM) which is the most recent commercially available PMEM on the market, researchers in \cite{empirical} explored properties and characteristics of the product at different levels. Optane memory has several features as below:
\begin{itemize}
\item Optane DIMM has higher density compared to DRAM, therefore, it is available in bigger capacities like 128, 256 and 512 BG per DIMM.
\item Optane DIMMs like traditional DRAM DIMMs sit on the memory bus, and connects to the processor's Integrated Memory Controller (iMC). iMC is responsible for maintaining the Optane DIMMs and DRAMs' functionalities.
\item Optane DIMMs operate in two modes: Memory, which CPU and OS simply see the DIMMs as a larger Volatile portion of main memory, and Direct mode, which behaves as a persistent memory.
\item Optane DIMM is both persistent and byte-addressable. It means that it can fill the role of either a main memory or a persistent device(e.g. SSD).
\end{itemize}
Applications access the Optane DIMM's content using store instructions which are the extended instruction Set Architecture (ISA), and those stores will become persistent.
PMEMs also introduce several new programming challenges such as:
\begin{itemize}
\item CPUs have out-of-order CPU execution and cache access/flushing.  This means if two values are stored by the application, the order in which they become persistent may not be the order that the application wrote them.
\item If power outage happens during an aligned 8 bytes store to PMEM, either the old 8 bytes or the new one will be found in that location after reboot. In other words, 8 bytes stores are powerfail atomic.
\item As anything larger than 8 bytes is not powerfail atomic, it is up to the software to implement whatever Transactions are required for consistency.
\item Memory leaks to persistent storage are persistent.  Rebooting the server doesn't change the on-device contents.
\item Since applications have direct access to the persistent memory media, any errors will be returned back to the application as memory errors. So, applications may need to detect and handle hardware errors directly.
\end{itemize}

According to Figure \ref{figure:hierarchy}, To get the low latency direct access, a new software architecture is required that allows applications to access ranges of persistent memory. The Persistent Memory Development Kit (PMDK) is a collection of libraries and tools for System Administrators and Application Developers to simplify managing and accessing persistent memory devices. The libraries are built on Direct Access (DAX) feature which allows applications to access persistent memory directly as memory-mapped file\cite{snia}. Support for this feature is what differentiates a normal file system from a persistent memory-aware file system. Figure \ref{figure:model} shows the model which describes how applications can access persistent memory devices (NVDIMMs) using traditional POSIX standard APIs such as read, write, pread, and pwrite, or load/store operations such as memcpy when the data is memory mapped to the application. For persistent memory, the APIs for memory mapping files are at the heart of the persistent memory programming model published by \cite{snia}. The 'Persistent Memory' area describes the fastest possible access because the application I/O bypasses existing filesystem page caches and goes directly to/from the persistent memory media.

![picture](data/pmdkdiagram.png)

Unfortunately, most persistent memories are designed for the single-node environment. Also in an empirical observation \cite{empirical} it is mentioned that the it is unclear scalable persistent memories will evolve. So, with modern datacenters applicatoins' computation scale, we have to be able to scale out persistent memory systems\cite{dspm} and hide background complexities from application developers by providing suitable abstractions.

### High-performance Networking
Datacenter network performance has improved significantly over the past decades. Infiniband (IB) NICs and switches support high bandwidth ranging from 40 to 100Gbps. Remote Direct Memory Access (RDMA) technologies that provide low-latency remote memory accesses have become more mature fro datacenter uses in recent years. Therefore, these network technology advances make remote-memory-based systems more attractive than decades ago\cite{dspm}. It is also commonly believed that datacenter networking software must sacrifice generality to attain high performance. For example, The result of an attempt to reach peak performance in communications inside datacenters has led to the emergence of Remote Direct Access Memory (RDMA). These specialized technologies were deployed with the belief that placing their functionality in the network will yield a large performance gain. But in \cite{eRPC} it is shown that a general purpose RPC can provide state-of-the-art performance on commodity datacener networks without additional networks support.

(RPC)
Remote Procedure Call (RPC) is not technically a protocol, but it is better thought of as a general mechanism for structuring distributed systems. RPC is popular because it is based on the semantics of a local procedure call. An application developer can be largely unaware of whether the procedure is local or remote.
![picture](data/rpc.png)
Disadvantages of RPC:
* Remote Procedure Call Passes Parameters by values only and pointer values, which is not allowed.
* Remote procedure calling (and return) time (i.e., overheads) can be significantly lower than that for a local procedure.
* This mechanism is highly vulnerable to failure as it involves a communication system, another machine, and another process.
* RPC concept can be implemented in different ways, which is can't standard.
* Not offers any flexibility in RPC for hardware architecture as It is mostly interaction-based.
* The cost of the process is increased because of a remote procedure call.

eRPC is a fast and general remote procedure call for datacenters. Modern datacenters are very fast, as we see 100Gbps datacenters with latency of 2microseconds between two hosts connected to the same switch, and switches add around 300nanosecond per hop. The problem in datacenter network is that existing networking options are trying to either sacrifice performance or generality. On the slow hand, we have TCP, gRPC, etc. which work in commodity datacenters and provide features needed to build applications like reliability and congestion control, but they are slow. On the other hand, we have fast but specialized options like DPDK and RDMA which make simplifying assumptions, while requires special hardware. One of the drawback in this field is limited applicability, which means we need specialized hardware to run them. But more fundamental drawback is that these systems co-design the application logic with the networking, and as a result, they lag modular networking abstraction which prevents reuse.
eRPC breaks away from this performance generality trade-off by providing both, that is **do not give up generality for high  performance**.
![picture](data/erpc_general.png)
 To do so, \cite{eRPC} have to provide functionality equivalent to hardware solutions in software. For example, because it is assumed that (IX) provides end to end reliability and congestion control, we have to do it in software but without loosing performance. There were three main challenges that this paper had to solve to build the eRPC:
* this paper assumes that the network does not prevent packet loss. Therefore, there is a need to manage packet loss in software.
* There is a need to implement transport layer end to end reliability and congestion control with low overhead.
*  Needs to support larger feature sets, so that eRPC can be easily integrated into existing application code.

So, here is how challenges are addressed:
##### 1) managing packet loss:
One problem in managing packet loss is **large timeouts**. When the drain rate is smaller than receiving rate, there will be packet loss. When this happens, the switch buffer starts filling up and when it is full, it starts dropping packets. Because the sender get no feedback from the network, it does not know when it is safe to re-transmit, so they use conservative approach for re-transmission which is in the order of miliseconds. RPC workloads commonly use small messages, so there is no need to rely on duplicate acknowledgements for faster re-transmission. Large timeouts are bad because they increase latency. For example, we're building a distributed system where clients hold locks on a remote object, if a client's unlock packets get dropped, the locked object still is locked for several miliseconds. As a result, contending requests from other clients will be failed, which leads to low performance. The prior solution for this problem was lossless link layer(PFC, Infiniband). In a lossless datacenter, before switch buffer fills up, the switch sends a feedback to the sender in a form of pause message, telling it to stop. So, in lossless datacenters, there is no need for re-transmission. However, link layer losslessness comes with trade-offs. Although it provides cheap and simple reliablility, it may cause Deadlock or unfairness. (Reliable link layer is an active field of research).

The approach which eRPC takes to solve this problem is that the researchers made an observation about normal datacenter networks that allows a simple method of preventing packet loss most of the time. Note that, there is no need to completely eliminate the packet loss. That is an extreme requirement which comes with drawbacks. what is needed for packet loss is to be rare enough to not affect the system performance, and this contribution has found that with a little help from software, existing network can satisfy this relaxed requirement.

There is a parameter which is called **Bandwidth Delay Product (BDP)** denotes the maximum size of sliding window to reach peak rate with available bandwidth:
```
BDP = Bandwidth * RTT
```
giving this information, in contrast, we see that in datacenter switches, there are around 12MB of buffer, which is 3 orders of magnitude larger than BDP. So, if software is limited to the amount of BDP, then switch can buffer hundreds of loads and prevents packet loss. So, this observation lets researchers to work on low latency NICs.

eRPC by limiting the window size to 19KB with 12MB of intermediate switch buffer, can achieve 640 nodes in many to one traffic pattern.

##### 2) Low overhead transport layer
Many transport layer components like end 2 end reliability, congestion control and memory management are quite expensive, but this paper has found that most of the overhead can be avoided in the common case. in eRPC, it is assumed that the datacenter network is uncongested, therefore, eRPC implementation is optimized for uncongested networks. There, it uses Timely algorithm to control congestion that is, if RTT is high, then TX_Rate will decrease and vice versa.

The reason for using eRPC for communication is that we want to implement our system with minimum alteration in hardware. For example, in order to have performance, RDMA is used instead of traditional TCP/IP stack. eRPC claims that it can propose system performance comparable to RDMA-based systems only by using traditional system softwares even in lossy environment such as ethernet. eRPC is optimized for common-case scenarios, so it is considered a general purpose communication system. As a result, I believe it can be opti-mized for special use cases.
eRPC implements RPCs on top of Transport Layer that pro-vides basic unreliable packet I/O, such as UDP.

### Security

One approach to enable security at lower level is to use trust-ed execution environments or enclaves. It can protect sensitive data and code even from powerful attackers that control or have compromised the OS and the hypervisor on a host ma-chine. One cummercial product is Intel SGX which provides security and protection by creating an enclave in applications's virtual address space. Once an enclave has been initialized, code and data within the enclave is isolated from the rest of the system including privileged software. Also, SGX protects enclaves against variaty of hardware/software attacks. SGX partitions the physical memory into two regions: Enclave Page Cache (EPC) that stores recently ac-cessed sensitive pages and non-EPC region that stores non-sensitive pages as well as sensitive pages spilled out of the EPC. While Intel SGX provides data confidentiality and in-tegrity by a feature called sealing, it does not provides data freshness. Generally, confidentiality is preserved by encrypting all signals that emerge from the processor. Integrity is a prop-erty that the memory that the memory system correctly returns the last-written block of data at any address\cite{11}. An applica-tion which needs secure environment in SGX-enabled CPUs, should be divided into two parts: A secure part which is launched inside enclave and non-secure part which resides out of the enclave. Enclave code and data are placed in a special memory area called Enclave Page Cache (EPC). This memory area is encrypted using Memory Encryption Engine (MEE), and pages are only decrypted when they are inside the physi-cal processor core. Keys are generated at boot-time and are stored within the CPU.



A recent research\cite{8} shows that key-value storage systems in cloud environment plays a significant role in storing online data. Cloud and high-performance computing requires data to be moved between hundreds or thousands of physical systems. Data may be replicated across multiple nodes for redundancy and protection, or can be split across physical systems for performance. Currently, high-performance networking fab-rics such as RDMA provides this interconnection, but accord-ing to \cite{9} instead of adding complexity to lower level in order to gain performance, examining the problem and adding abstractions to higher layer of the system without altering under-lying infrastructure will gain better or same performance. Also, the emergence of persistent memory have starts a new trend in storage systems. The need for data replication, protection and even reducing access time with lower cost is growing fast. However, these needs have come up with various challenges. One problem is that there is no guranatee to ensure that re-mote write to persistent memory is stored safely. This is be-cause each OS, network/fabric and devices has its own dura-bility guarantee. Therefore, there is a need for suitable abstrac-tion for remote persistent memory.
In addition, it has become so important that KV stores must be fortified with different security measures. However, as modern storage systems have become quite sophisticated, se-curing those systems are quite challenging. Storage systems security are mostly implemented in upper layers, and there are vulnerabilities which can be experience in lower layers. There-fore, attackers might find ways to intrude the systems. So, ensuring lower level security properties such as confidentiality, integrity and freshness are required\cite{10}.  







NVMMs appear to mesh perfectly with another popular technology, remote direct memory access (RDMA). RDMA gives a client direct access to memory on a remote machine and mediates this access through a memory region abstraction that handles the necessary translations and permissions. NVMM and RDMA seem eminently compatible: by combining them, we should be able to build network-attached, byte-addressable, persistent storage. Unfortunately, however, the systems were not designed to work together. An NVMM- aware file system manages persistent memory as files, whereas RDMA uses a different abstraction memory regions to organize remotely accessible memory. As a result, in practice, building RDMA-accessible NVMMs requires expensive translation layers resulting from this duplication of effort that spans permissions, naming, and address translation. At first glance, by combining NVMM and RDMA, we could unify storage, memory and network to provide large, stable, byte-addressable network-attached memory. Unfortunately, the existing systems used to manage these technologies are simultaneously overlapping and incompatible.  RDMA merges local and remote memory. RDMA allows a client to directly access memory on a remote server. Once the remote server decides to allow incoming access, it registers a portion of its address space as an RDMA memory region and sends the client a key to access it. Using the key, the client can enlist the server’s RDMA network interface (RNIC) to directly read and write to the server’s memory, bypassing the CPU. RDMA is popular as it offloads most of the networking stack onto hardware and provides close-to-hardware abstractions, exhibiting much better latency compared to TCP/IP protocol.

RPMA – Remote Persistent Memory Access

Latency is a key consideration in choosing a connectivity method for memory or storage. Latency refers to the time it takes to complete an access such as a read, write, load or store. There are two very important latency thresholds that change how applications see storage or memory represented by the background color bands in this figure. These thresholds are used by system designers when implementing access to stored data, to determine whether the access is to be synchronous, polled or asynchronous. In today’s large non-uniform memory access (NUMA) systems, latencies of up to 200 nS are generally considered to be acceptable. NUMA systems must be very responsive because CPU instruction processing on a core or thread is suspended during the memory access. Latencies of more than 200 nS in a memory system quickly pile up, resulting in wasted CPU time. On the other hand, when an application does IO for a storage access that is expected to take more than 2-3 uS, it will usually choose to block a thread or process. The CPU will execute a context switch and make progress on another thread or process until it is notified that the access is complete. For latencies between 200 nS and 2 uS it may be preferable for the CPU to poll for IO completion as this consumes one thread or core but does not slow down the rest of the CPU.

The concept of disaggregated memory is used to illustrate cases where memory that is not contained within a server is still accessed at memory speed. Disaggregated memory still looks like memory to CPU’s. It operates at memory speed in cache line size units and it may be subject to distance limitations to insure sufficiently low latency. Disaggregated memory is made scalable through the use of optical networks such as those based on silicon photonics to increase the distance of memory speed interfaces.
Networked memory is accessed through a high speed network rather than directly through a memory interface. Memory access is achieved over the network using protocols such as message passing and RDMA. Networked persistent memory is not cache coherent with the CPU. Unlike local or disaggregated persistent memory where all of the NVDIMMs can be part of a single system image, the NVDIMMs on a remote node are not part of a single system image.

![picture](data/model.png)
