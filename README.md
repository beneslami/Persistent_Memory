# Persistent_Memory

## Introduction

Application performance and security are undeniable parts of high-performance systems in datacenters. Hence, there are numerous number of studies in various layers of these systems that some of them are in infancy phase \textcolor{red}{\cite{}} and rest of them are commercialized \textcolor{red}{\cite{}}. In addition, distributed high-performance systems such as database systems or key-value stores which are widely used in production datacenters, need to be improved in various aspects \textcolor{red}{\cite{}}. For example, there should be quite huge, byte-addressable, persistence and low cost memory systems in high-performance datacenters for applications like distributed databases. Also, there should be a secure, reliable and trusted mechanism to completely isolate applications' code and data from each other to prevent the occurrence possible attacks. According to Recent researches in \cite{intelsgx,empirical}, Intel proposed two products to address above mentioned issues: Intel Optane Persistent Memory (PMEM) and Intel Software Guard Extension (SGX). Intel Optane Persistent Memory (PMEM) promises byte-addressability, persistence, high capacity (128/256/512GB per DIMM), lower latency in read/write, lower cost than DRAMs and high performance, all on memory bus \cite{enclavedb, occlum}. Intel SGX enables user-level code to create private memory regions called enclaves, which code and data are protected by the CPU from any software and hardware attacks outside the enclaves. In fact, SGX provides a practical solution to the problem of secure computation on untrusted platforms such as multi tenant datacenters and public clouds \cite{occlum}.

Furthermore, with the emergence of Remote Persistent Memory (RPM), the developers have been studied to implement high-performance distributed systems on RPM with specific remote memory access, called Remote Direct Memory Access (RDMA). RDMA-based distributed systems use specific kinds of NICs, called RNIC which eliminates TCP/IP protocol stack for network connectivity. Therefore, it is not possible to implement high-performance distributed systems over commodity hardware. The problem also gets even more complex when these systems need strong security mechanisms to have confidentiality, integrity and freshness in untrusted environments. Some researches such as \cite{eRPC} and \cite{dspm} proposed mechanisms for distributed systems to access resources remotely with latency comparable to specialized hardware like RDMA. Yet, there are several works need to be added in these mechanisms such as security to prepare the whole system operable in un-trustable environments. As a result, there is a fundamental need to provide an infrastructure for developers to implement a secure and distributed high performance system over commodity hardware inside datacenters. So, the ultimate goal in proposed research is to design and build COSMOS, a Se\underline{c}ure Rem\underline{o}te Per\underline{s}istent Me\underline{mo}ry Ab\underline{s}traction, which can be used to build different set of high-performance and secure applications such as KV stores/databases accessible over the traditional TCP/IP network inside datacenters. To this end, the entire system is divided into three parts: Persistent Memory, Networking infrastructure which leverages the concept and implementation of Extended Remote Procedure Call (eRPC) and security which is provided by Intel SGX.

## Background

In this section, each part of COSMOS which is mentioned in the previous section, will be elaborated in greater detail.

### Persistent Memory

Persistent Memory or PMEM is a type of Non-Volatile memory (NVMem) which is directly connected to memory bus rather than PCI-e. PMEMs provide byte-addressability, persistence, and latency which is within an order of magnitude of DRAM\cite{dspm, empirical,snia}.

With the emergence of Intel's Optane Dual Inline Memory Module (DIMM) which is the most recent commercially available PMEM on the market, researchers in \cite{empirical} explored properties and characteristics of the product at different levels. Optane memory has several features as below:
\begin{itemize}
\item Optane DIMM has higher density compared to DRAM, therefore, it is available in bigger capacities like 128, 256 and 512 BG per DIMM.
\item Optane DIMMs like traditional DRAM DIMMs sit on the memory bus, and connects to the processor's Integrated Memory Controller (iMC). iMC is responsible for maintaining the Optane DIMMs and DRAMs' functionalities.
\item Optane DIMMs operate in two modes: Memory, which CPU and OS simply see the DIMMs as a larger Volatile portion of main memory, and Direct mode, which behaves as a persistent memory.
\item Optane DIMM is both persistent and byte-addressable. It means that it can fill the role of either a main memory or a persistent device(e.g. SSD).
\end{itemize}

Applications access the Optane DIMM's content using store instructions which are the extended instruction Set Architecture (ISA), and those stores will become persistent.

Unfortunately, most persistent memories are designed for the single-node environment. Also in an empirical observation \cite{empirical} it is mentioned that the it is unclear scalable persistent memories will evolve. So, with modern datacenters applicatoins' computation scale, we have to be able to scale out persistent memory systems\cite{dspm}.



Among available libraries in PMDK, there are two libraries which there is a need in implementing proposed system: libpmem and librp-mem. libpmem which is small and fairly simple, provides low level persistent memory support. The library also provides performance-tuned routines for copying ranges of persistence memory using the best instruction choices for the platform. librpmem provides low level support for for remote access to persistent memory using RDMA-capable RNICs. The library is capable of replicating a memory region remotely via RDMA protocol.



### High-performance Networking

The reason for using eRPC for communication is that we want to implement our system with minimum alteration in hardware. For example, in order to have performance, RDMA is used instead of traditional TCP/IP stack. eRPC claims that it can propose system performance comparable to RDMA-based systems only by using traditional system softwares even in lossy environment such as ethernet. eRPC is optimized for common-case scenarios, so it is considered a general purpose communication system. As a result, I believe it can be opti-mized for special use cases.
eRPC implements RPCs on top of Transport Layer that pro-vides basic unreliable packet I/O, such as UDP.

### Security

One approach to enable security at lower level is to use trust-ed execution environments or enclaves. It can protect sensitive data and code even from powerful attackers that control or have compromised the OS and the hypervisor on a host ma-chine. One cummercial product is Intel SGX which provides security and protection by creating an enclave in applications's virtual address space. Once an enclave has been initialized, code and data within the enclave is isolated from the rest of the system including privileged software. Also, SGX protects enclaves against variaty of hardware/software attacks. SGX partitions the physical memory into two regions: Enclave Page Cache (EPC) that stores recently ac-cessed sensitive pages and non-EPC region that stores non-sensitive pages as well as sensitive pages spilled out of the EPC. While Intel SGX provides data confidentiality and in-tegrity by a feature called sealing, it does not provides data freshness. Generally, confidentiality is preserved by encrypting all signals that emerge from the processor. Integrity is a prop-erty that the memory that the memory system correctly returns the last-written block of data at any address\cite{11}. An applica-tion which needs secure environment in SGX-enabled CPUs, should be divided into two parts: A secure part which is launched inside enclave and non-secure part which resides out of the enclave. Enclave code and data are placed in a special memory area called Enclave Page Cache (EPC). This memory area is encrypted using Memory Encryption Engine (MEE), and pages are only decrypted when they are inside the physi-cal processor core. Keys are generated at boot-time and are stored within the CPU.



A recent research\cite{8} shows that key-value storage systems in cloud environment plays a significant role in storing online data. Cloud and high-performance computing requires data to be moved between hundreds or thousands of physical systems. Data may be replicated across multiple nodes for redundancy and protection, or can be split across physical systems for performance. Currently, high-performance networking fab-rics such as RDMA provides this interconnection, but accord-ing to \cite{9} instead of adding complexity to lower level in order to gain performance, examining the problem and adding abstractions to higher layer of the system without altering under-lying infrastructure will gain better or same performance. Also, the emergence of persistent memory have starts a new trend in storage systems. The need for data replication, protection and even reducing access time with lower cost is growing fast. However, these needs have come up with various challenges. One problem is that there is no guranatee to ensure that re-mote write to persistent memory is stored safely. This is be-cause each OS, network/fabric and devices has its own dura-bility guarantee. Therefore, there is a need for suitable abstrac-tion for remote persistent memory.
In addition, it has become so important that KV stores must be fortified with different security measures. However, as modern storage systems have become quite sophisticated, se-curing those systems are quite challenging. Storage systems security are mostly implemented in upper layers, and there are vulnerabilities which can be experience in lower layers. There-fore, attackers might find ways to intrude the systems. So, ensuring lower level security properties such as confidentiality, integrity and freshness are required\cite{10}.  







NVMMs appear to mesh perfectly with another popular technology, remote direct memory access (RDMA). RDMA gives a client direct access to memory on a remote machine and mediates this access through a memory region abstraction that handles the necessary translations and permissions. NVMM and RDMA seem eminently compatible: by combining them, we should be able to build network-attached, byte-addressable, persistent storage. Unfortunately, however, the systems were not designed to work together. An NVMM- aware file system manages persistent memory as files, whereas RDMA uses a different abstraction memory regions to organize remotely accessible memory. As a result, in practice, building RDMA-accessible NVMMs requires expensive translation layers resulting from this duplication of effort that spans permissions, naming, and address translation. At first glance, by combining NVMM and RDMA, we could unify storage, memory and network to provide large, stable, byte-addressable network-attached memory. Unfortunately, the existing systems used to manage these technologies are simultaneously overlapping and incompatible.  RDMA merges local and remote memory. RDMA allows a client to directly access memory on a remote server. Once the remote server decides to allow incoming access, it registers a portion of its address space as an RDMA memory region and sends the client a key to access it. Using the key, the client can enlist the server’s RDMA network interface (RNIC) to directly read and write to the server’s memory, bypassing the CPU. RDMA is popular as it offloads most of the networking stack onto hardware and provides close-to-hardware abstractions, exhibiting much better latency compared to TCP/IP protocol.

RPMA – Remote Persistent Memory Access

Latency is a key consideration in choosing a connectivity method for memory or storage. Latency refers to the time it takes to complete an access such as a read, write, load or store. There are two very important latency thresholds that change how applications see storage or memory represented by the background color bands in this figure. These thresholds are used by system designers when implementing access to stored data, to determine whether the access is to be synchronous, polled or asynchronous. In today’s large non-uniform memory access (NUMA) systems, latencies of up to 200 nS are generally considered to be acceptable. NUMA systems must be very responsive because CPU instruction processing on a core or thread is suspended during the memory access. Latencies of more than 200 nS in a memory system quickly pile up, resulting in wasted CPU time. On the other hand, when an application does IO for a storage access that is expected to take more than 2-3 uS, it will usually choose to block a thread or process. The CPU will execute a context switch and make progress on another thread or process until it is notified that the access is complete. For latencies between 200 nS and 2 uS it may be preferable for the CPU to poll for IO completion as this consumes one thread or core but does not slow down the rest of the CPU.

The concept of disaggregated memory is used to illustrate cases where memory that is not contained within a server is still accessed at memory speed. Disaggregated memory still looks like memory to CPU’s. It operates at memory speed in cache line size units and it may be subject to distance limitations to insure sufficiently low latency. Disaggregated memory is made scalable through the use of optical networks such as those based on silicon photonics to increase the distance of memory speed interfaces.
Networked memory is accessed through a high speed network rather than directly through a memory interface. Memory access is achieved over the network using protocols such as message passing and RDMA. Networked persistent memory is not cache coherent with the CPU. Unlike local or disaggregated persistent memory where all of the NVDIMMs can be part of a single system image, the NVDIMMs on a remote node are not part of a single system image.

![picture](data/model.png)
